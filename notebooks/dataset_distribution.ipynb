{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nlq_train = pd.read_csv('/data/gunsbrother/prjs/ltvu/ours/data/Ego4D/EgoNLQ/csvs/nlq_train_v2.csv')\n",
    "nlq_val = pd.read_csv('/data/gunsbrother/prjs/ltvu/ours/data/Ego4D/EgoNLQ/csvs/nlq_val_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['video_uid', 'clip_uid', 'video_start_sec', 'video_end_sec',\n",
      "       'video_start_frame', 'video_end_frame', 'clip_start_sec',\n",
      "       'clip_end_sec', 'clip_start_frame', 'clip_end_frame', 'duration_sec',\n",
      "       'duration_frame', 'annotation_uid', 'q_video_start_sec',\n",
      "       'q_video_end_sec', 'q_video_start_frame', 'q_video_end_frame',\n",
      "       'q_clip_start_sec', 'q_clip_end_sec', 'template', 'query', 'slot_x',\n",
      "       'verb_x', 'slot_y', 'verb_y', 'q_clip_start_frame', 'q_clip_end_frame',\n",
      "       'q_duration_sec', 'q_duration_frame', 'q_coverage', 'query_idx',\n",
      "       'q_uid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(nlq_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns\n",
    "train_question = list(nlq_train['query'])\n",
    "val_question = list(nlq_val['query'])\n",
    "duplicated_question = list(set(train_question) & set(val_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train questions: 13847\n",
      "# of val questions: 4552\n",
      "# of duplicated questions: 812\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of train questions: {len(train_question)}\")\n",
    "print(f\"# of val questions: {len(val_question)}\")\n",
    "print(f\"# of duplicated questions: {len(duplicated_question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_verb_x = list(nlq_train['verb_x'])\n",
    "train_verb_y = list(nlq_train['verb_y'])\n",
    "train_slot_x = list(nlq_train['slot_x'])\n",
    "train_slot_y = list(nlq_train['slot_y'])\n",
    "\n",
    "train_verb = list(map(lambda x, y: f\"{x} {y}\", train_verb_x, train_verb_y))\n",
    "train_verb = list(set(train_verb))\n",
    "train_slot = list(map(lambda x, y: f\"{x} {y}\", train_slot_x, train_slot_y))\n",
    "train_slot = list(set(train_slot))\n",
    "\n",
    "val_verb_x = list(nlq_val['verb_x'])\n",
    "val_verb_y = list(nlq_val['verb_y'])\n",
    "val_slot_x = list(nlq_val['slot_x'])\n",
    "val_slot_y = list(nlq_val['slot_y'])\n",
    "\n",
    "val_verb = list(map(lambda x, y: f\"{x} {y}\", val_verb_x, val_verb_y))\n",
    "val_verb = list(set(val_verb))\n",
    "val_slot = list(map(lambda x, y: f\"{x} {y}\", val_slot_x, val_slot_y))\n",
    "val_slot = list(set(val_slot))\n",
    "\n",
    "duplicated_verb = list(set(train_verb) & set(val_verb))\n",
    "duplicated_slot = list(set(train_slot) & set(val_slot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train verbs: 415\n",
      "# of val verbs: 271\n",
      "# of train slots: 7893\n",
      "# of val slots: 3007\n",
      "# of duplicated verbs: 206\n",
      "# of duplicated slots: 826\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of train verbs: {len(train_verb)}\")\n",
    "print(f\"# of val verbs: {len(val_verb)}\")\n",
    "\n",
    "print(f\"# of train slots: {len(train_slot)}\")\n",
    "print(f\"# of val slots: {len(val_slot)}\")\n",
    "\n",
    "print(f\"# of duplicated verbs: {len(duplicated_verb)}\")\n",
    "print(f\"# of duplicated slots: {len(duplicated_slot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "train_annotation = json.loads(Path('/data/soyeonhong/GroundVQA/data/unified/annotations.NLQ_train.json').read_text())\n",
    "val_annotation = json.loads(Path('/data/soyeonhong/GroundVQA/data/unified/annotations.NLQ_val.json').read_text())\n",
    "\n",
    "def replace_func(text):\n",
    "    # Define the pattern to match \".\" and \"?\"\n",
    "    pattern = r'[.?]|Text:\\w+|Query|Text:'\n",
    "    # Replace occurrences of the pattern with an empty string\n",
    "    result = re.sub(pattern, '', text)\n",
    "    return result\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train words: 2707\n",
      "# of val words: 1578\n",
      "# of duplicated words: 1248\n",
      "# of no duplicated words: 330\n"
     ]
    }
   ],
   "source": [
    "train_word = []\n",
    "val_word = []\n",
    "\n",
    "for anno in train_annotation:\n",
    "    for word in anno['question'].split():\n",
    "        edited_word = replace_func(word).lower()\n",
    "        if not (edited_word in stop_words or edited_word == ''):\n",
    "            train_word.append(edited_word)\n",
    "        \n",
    "for anno in val_annotation:\n",
    "    for word in anno['question'].split():\n",
    "        edited_word = replace_func(word).lower()\n",
    "        if not (edited_word in stop_words or edited_word == ''):\n",
    "            val_word.append(edited_word)\n",
    "\n",
    "train_word_counter = Counter(train_word)\n",
    "val_word_counter = Counter(val_word)\n",
    "        \n",
    "train_word = list(set(train_word))\n",
    "val_word = list(set(val_word))\n",
    "duplicated_word = list(set(train_word) & set(val_word))\n",
    "no_duplicated_word = list(set(val_word) - set(train_word))\n",
    "\n",
    "print(f\"# of train words: {len(train_word)}\")\n",
    "print(f\"# of val words: {len(val_word)}\")\n",
    "print(f\"# of duplicated words: {len(duplicated_word)}\")\n",
    "print(f\"# of no duplicated words: {len(no_duplicated_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of no duplicated words in val: 429\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "for anno in val_annotation:\n",
    "    \n",
    "    for word in anno['question'].split():\n",
    "        edited_word = replace_func(word).lower()\n",
    "        if not edited_word in stop_words:\n",
    "            if edited_word in no_duplicated_word:\n",
    "                query.append(anno['question'])\n",
    "                # print(anno['question'])\n",
    "                # print(f\"{edited_word}\\n\")\n",
    "\n",
    "print(f\"# of no duplicated words in val: {len(query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cork', 'pamphlet', 'cot', 'chilli', 'tilt', 'form', 'women', 'coffeemaker', 'funnels', 'furry', 'handtowel', 'sledge', 'chase', 'lampstand', 'markings', 'coats', 'machining', 'transferred', 'straightener', 'wireless', 'dreamers', 'method', 'pantry', 'lollipops', 'posters', 'robot', 'jumper', 'initially', 'insulated', 'sling', 'thickness', 'woolen', 'tacks', 'flop', 'hint', 'cautions', 'gel', 'ortega', 'patches', 'wass', 'find', 'swapping', 'pies', 'whose', 'knock', 'soy', 'underneath', 'shed', 'connect4', 'knitted', 'cold', 'roofing', 'lightener', 'fetched', 'greet', 'blend', \"plier'\", 'kettles', 'sandy', 'locking', 'searchlight', 'galore', 'width', 'meter', 'wave', 'hammered', 'lunchbox', 'rocky', 'scooping', 'rulers', 'yolk', \"door's\", 'tins', 'same-sized', 'degree', 'batter', 'coupled', 'sealant', 'chord', 'loan', 'unscrewing', 'herb', 'sweaters', 'handed', 'okro', 'genre', 'bran', 'crossed', 'poly', 'pathways', 'dresses', 'berries', 'turbine', 'cartridges', 'cpu', 'fevicol', 'drums', 'alphabet', 'signboard', 'chisels', 'cellotapes', 'bicylce', 'bits', 'micro', 'yum', 'hugging', 'along', 'chases', 'purses', 'radio', 'blouses', 'reef', 'color-cloth', 'brash', 'facemasks', 'collar', 'presses', 'disconnect', 'dolly', 'risk', 'tabel', 'handkerchiefs', 'silhouette', 'clour', 'l-square', 'french', 'a1', 'tales', 'keyboards', 'hoop', 'cassette', 'buttons', 'leveler', 'stowed', 'strands', \"can's\", 'circuit', 'pant', 'rices', 'communicate', 'trowels', 'frontage', 'crochet', 'tunnel', 'photographed', 'tostitos', 'bundle', 'connector', 'jerk', 'wallpapers', 'rider', 'tupperware', 'identification', 'metre', 'knead', 'tom', 'turquoise', 'gravity', 'tents', 'end', 'textbook', 'brace', 'platform', 'adding', 't-spanner', 'guns', 'tortilla', 'drawstring', 'broth', 'clamping', 'whree', 'pandal', 'boiler', 'custom', 'world', 'objects', 'poker', 'edge', 'croutons', 'leftover', 'scraping', \"hoodie's\", \"'wire\", 'stripping', 'support', 'observed', 'opposite', 'cart\"', 'ariel', 'restroom', 'defenders', 'cabbages', 'smear', 'sprayed', 'popcorn', 'masks', 'wherr', 'spot', 'knot', 'lintel', 'bricked', 'queso', 'specks', 'skated', 'breaker', 'article', 'shovels', 'bamboo', 'trimmed', 'vaping', 'aisle', 'footmat', 'kid', 'stopper', 'sarong', 'lube', 'litterbin', 'unlock', 'unauthorized', 'air-blower', 'ion', 'spacer', 'hand-held', '\"stop\"', 'springer', 'cultleries', 'cultlery', 'pastries', 'set-square', 'duvet', 'groceries', 'surrounded', 'locked', 'fryer', 'texting', 'four', 'sculpture', 'change', 'knit', 'ear', 'bedding', 'handful', 'herdez', 'frisbee', 'layer', 'drillers', 'claw', 'asparagus', 'bookshelf', 'celery', 'chunck', 'l-shaped', 'batteries', 'symbol', 'hoodie', 'brass', 'wedges', 'tuk-tuk', 'served', 'cigarettes', 'plaster', 'position', 'stands', 'away', 'cow', 'cooling', 'rotated', 'extra', 'steam', 'protector', 'paying', 'lumber', 'eraser', 'normal', 'attaching', 'becnh', 'repair', 'whisked', 'leaned', 'pointing', 'buns', 'pillowcase', 'candies', 'cornflakes', 'us', 'cauliflower', \"dog's\", 'cafe', 'run', 'golden', 'mopping', 'seasoned', 'yora', 'leek', 'multifiss', 'basins', 'shades', 'bale', 'burrito', 'total', 'mango', 'wand', 'sunglass', 'smelled', 'parchment', 'luggage', 'ash', 'entry', 'till', 'hug', 'skating', 'queued', 'mac', 'counterfoil', 'harpic', 'machete', 'refuse', 'wastebin', 'stakes', 'code', 'portions', 'bluetooth', 'menu', 'trainers', '-end', 'marble', 'placing', 'xl', 'sharpener', 'mud']\n"
     ]
    }
   ],
   "source": [
    "print(no_duplicated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106166"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {\"train\": train_word, \"val\": val_word, \"duplicated\": duplicated_word, \"no_duplicated\": no_duplicated_word}\n",
    "\n",
    "Path('/data/soyeonhong/GroundVQA/word_dict.json').write_text(json.dumps(word_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_json = json.loads(Path('/data/soyeonhong/GroundVQA/word_dict.json').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
