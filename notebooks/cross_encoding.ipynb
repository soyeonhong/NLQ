{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/soyeonhong/anaconda3/envs/videollava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "cross_encoder = CrossEncoder(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "llava_result_list = Path('/data/soyeonhong/GroundVQA/llava-v1.6-34b/global/').glob('*.json')\n",
    "nlq_list = json.loads(Path(\"/data/soyeonhong/GroundVQA/data/unified/annotations.NLQ_val.json\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_id': 'f06d1935-550f-4caa-909c-b2db4c28f599',\n",
       " 'sample_id': 'd5513548-e16e-486d-8def-740ea7b7fbb0_0',\n",
       " 'question': 'what did I pick from the fridge?',\n",
       " 'moment_start_frame': 517.7007,\n",
       " 'moment_end_frame': 817.6800000000001,\n",
       " 'clip_start_sec': 17.25669,\n",
       " 'clip_end_sec': 27.256,\n",
       " 'clip_duration': 480.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlq_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_list = ['70a350cd-4f32-40ed-80dd-23a48e7c4e46',\n",
    "               '04199001-307a-40fd-b20c-4b4128546b89'] # nlq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation in nlq_list:\n",
    "    video_id = annotation['video_id']\n",
    "    sample_id = annotation['sample_id']\n",
    "    question = annotation['question']\n",
    "    if not video_id in except_list:\n",
    "        llava_caption_list = json.loads(Path(f\"/data/soyeonhong/GroundVQA/llava-v1.6-34b/global/{video_id}.json\").read_text())['answers']\n",
    "        \n",
    "        save_data = []\n",
    "        \n",
    "        for caption in llava_caption_list:\n",
    "            time = caption[0]\n",
    "            # print(time)\n",
    "            tok_output = tokenizer([caption[2], question], max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tok_output)\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                \n",
    "            temp_list = [caption[0],\n",
    "                         tuple(token_embeddings.shape),\n",
    "                         token_embeddings[0],\n",
    "                         token_embeddings[1]]\n",
    "                \n",
    "            save_data.append(temp_list)\n",
    "            \n",
    "        torch.save(save_data, f'cross_encoding/{sample_id}.pt')\n",
    "        print(f\"{sample_id}.pt is saved\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 89, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"/data/soyeonhong/GroundVQA/cross_encoding/val/0a454217-ecf1-4bb0-9365-30eed8e80c1d_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " (2, 128, 768),\n",
       " tensor([[-0.0646, -0.0799,  0.0160,  ...,  0.0276, -0.1183, -0.0521],\n",
       "         [-0.0160, -0.1267, -0.0466,  ...,  0.0920, -0.0075, -0.1555],\n",
       "         [ 0.1011, -0.2076, -0.0928,  ...,  0.0954,  0.0239, -0.0728],\n",
       "         ...,\n",
       "         [-0.0353,  0.0277,  0.0035,  ...,  0.0526, -0.0359, -0.0016],\n",
       "         [ 0.0339,  0.1140, -0.0056,  ..., -0.0590, -0.1598, -0.0460],\n",
       "         [ 0.0335, -0.0103, -0.0057,  ...,  0.0046, -0.1473, -0.0330]]),\n",
       " tensor([[ 0.1269, -0.2565,  0.0155,  ...,  0.0682, -0.0235,  0.0202],\n",
       "         [ 0.2156, -0.2340, -0.0227,  ...,  0.1586,  0.0737,  0.0628],\n",
       "         [ 0.1833, -0.2544,  0.0175,  ...,  0.1519, -0.0108,  0.0740],\n",
       "         ...,\n",
       "         [ 0.2280, -0.0289, -0.0055,  ...,  0.1417, -0.0306, -0.0196],\n",
       "         [ 0.2280, -0.0289, -0.0055,  ...,  0.1417, -0.0306, -0.0196],\n",
       "         [ 0.2280, -0.0289, -0.0055,  ...,  0.1417, -0.0306, -0.0196]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0: caption -> [128, 768], query -> [128, 768]\n",
      "   300: caption -> [99, 768], query -> [99, 768]\n",
      "   600: caption -> [115, 768], query -> [115, 768]\n",
      "   900: caption -> [142, 768], query -> [142, 768]\n",
      "  1200: caption -> [131, 768], query -> [131, 768]\n",
      "  1500: caption -> [99, 768], query -> [99, 768]\n",
      "  1800: caption -> [132, 768], query -> [132, 768]\n",
      "  2100: caption -> [106, 768], query -> [106, 768]\n",
      "  2400: caption -> [113, 768], query -> [113, 768]\n",
      "  2700: caption -> [143, 768], query -> [143, 768]\n",
      "  3000: caption -> [129, 768], query -> [129, 768]\n",
      "  3300: caption -> [106, 768], query -> [106, 768]\n",
      "  3600: caption -> [121, 768], query -> [121, 768]\n",
      "  3900: caption -> [100, 768], query -> [100, 768]\n",
      "  4200: caption -> [113, 768], query -> [113, 768]\n",
      "  4500: caption -> [105, 768], query -> [105, 768]\n",
      "  4800: caption -> [120, 768], query -> [120, 768]\n",
      "  5100: caption -> [81, 768], query -> [81, 768]\n",
      "  5400: caption -> [126, 768], query -> [126, 768]\n",
      "  5700: caption -> [106, 768], query -> [106, 768]\n",
      "  6000: caption -> [88, 768], query -> [88, 768]\n",
      "  6300: caption -> [104, 768], query -> [104, 768]\n",
      "  6600: caption -> [100, 768], query -> [100, 768]\n",
      "  6900: caption -> [95, 768], query -> [95, 768]\n",
      "  7200: caption -> [87, 768], query -> [87, 768]\n",
      "  7500: caption -> [138, 768], query -> [138, 768]\n",
      "  7800: caption -> [93, 768], query -> [93, 768]\n",
      "  8100: caption -> [132, 768], query -> [132, 768]\n",
      "  8400: caption -> [77, 768], query -> [77, 768]\n",
      "  8700: caption -> [86, 768], query -> [86, 768]\n",
      "  9000: caption -> [108, 768], query -> [108, 768]\n",
      "  9300: caption -> [124, 768], query -> [124, 768]\n",
      "  9600: caption -> [116, 768], query -> [116, 768]\n",
      "  9900: caption -> [86, 768], query -> [86, 768]\n",
      " 10200: caption -> [104, 768], query -> [104, 768]\n",
      " 10500: caption -> [118, 768], query -> [118, 768]\n",
      " 10800: caption -> [96, 768], query -> [96, 768]\n",
      " 11100: caption -> [127, 768], query -> [127, 768]\n",
      " 11400: caption -> [102, 768], query -> [102, 768]\n",
      " 11700: caption -> [111, 768], query -> [111, 768]\n",
      " 12000: caption -> [124, 768], query -> [124, 768]\n",
      " 12300: caption -> [121, 768], query -> [121, 768]\n",
      " 12600: caption -> [132, 768], query -> [132, 768]\n",
      " 12900: caption -> [111, 768], query -> [111, 768]\n",
      " 13200: caption -> [113, 768], query -> [113, 768]\n",
      " 13500: caption -> [110, 768], query -> [110, 768]\n",
      " 13800: caption -> [118, 768], query -> [118, 768]\n",
      " 14100: caption -> [106, 768], query -> [106, 768]\n",
      " 14400: caption -> [117, 768], query -> [117, 768]\n"
     ]
    }
   ],
   "source": [
    "for frame_idx, (count, num_tokens, token_embeddings), caption_feature, query_feature in x:\n",
    "    print(f'{frame_idx:6d}: caption -> {list(caption_feature.shape)}, query -> {list(query_feature.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
